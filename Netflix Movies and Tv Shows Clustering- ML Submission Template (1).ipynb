{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1709728549962}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","O_i_v8NEhb9l","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","g-ATYxFrGrvw","Yfr_Vlr8HBkt","8yEUt7NnHlrM","tEA2Xm5dHt1r","I79__PHVH19G","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","bn_IUdTipZyH","49K5P_iCpZyH","Nff-vKELpZyI","kLW572S8pZyI","dWbDXHzopZyI","yLjJCtPM0KBk","xiyOF9F70UgQ","7wuGOrhz0itI","id1riN9m0vUs","578E2V7j08f6","89xtkJwZ18nB","67NQN5KX2AMe","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","-oLEiFgy-5Pf","C74aWNz2AliB","2DejudWSA-a0","pEMng2IbBLp7","rAdphbQ9Bhjc","TNVZ9zx19K6k","nqoHp30x9hH9","rMDnDkt2B6du","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","VfCC591jGiD4","OB4l2ZhMeS1U","ArJBuiUVfxKd","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","dJ2tPlVmpsJ0","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","Fze-IPXLpx6K","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    -  Netflix Movies and Tv Shows Clustering\n","\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n","##### **Contribution**    - Individual/Team\n","##### **Team Member 1 -**\n","##### **Team Member 2 -**\n","##### **Team Member 3 -**\n","##### **Team Member 4 -**"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["The Netflix Movies and TV Shows Clustering project aimed to categorize similar movies and TV shows available on Netflix into distinct clusters based on their content. The project began with data collected from a third-party Netflix search engine, encompassing information about over 7,000 movies and TV shows available on the platform.\n","\n","The dataset consisted of 12 columns and 7,787 rows, with no duplicate values. However, certain columns such as director, cast, country, date added, and rating contained null values. Approximately 30.68%, 9.22%, 6.51%, 0.13%, and 0.09% of their respective features had null values. To handle this, null values were replaced with \"Unavailable\" for director, \"Cast Unavailability\" for cast, and \"Country Unavailable\" for country. For date_added and rating, which had minimal null values, these entries were eliminated.\n","\n","Initial data preprocessing involved converting the date_added feature to datetime and extracting additional features such as year added, month added, and day added. The release year feature's datatype was changed from float64 to int64, and outliers were treated using the interquartile range method.\n","\n","Exploratory data analysis included univariate, bivariate, and multivariate analyses, leading to several insights:\n","\n","1. Netflix offers more movies (69.14%) than TV shows (30.86%).\n","2. Majority of Netflix content was released between 2015 and 2020, with the peak years for movies being 2017 and for TV shows being 2020.\n","3. Netflix shifted focus towards TV shows over movies, particularly evident in the higher number of TV show releases in 2020 and 2021.\n","4. TV-MA and TV-14 are the most prevalent ratings for Netflix content.\n","5. Content addition peaked in 2019, with October and January being the months with the highest content additions.\n","6. The United States is the primary producer of Netflix movies, while the United States and the United Kingdom produce the most TV shows.\n","7. Raul Campos and Jan Suter directed most Netflix movies, while Alastair Fothergill directed the majority of TV shows.\n","8. International movies and dramas are popular genres on Netflix.\n","9. Common actors in Netflix content include Lee, Michel, David, Jhon, and James.10. Release year and month added are correlated, indicating consistent content additions throughout the year.\n","\n","\n","Text data from the description variable underwent preprocessing, including removing punctuation, stopwords, URLs, and digits, followed by lemmatization and TFIDF vectorization for model input. Various clustering algorithms were trained, including KMeans, Hierarchical clustering, and DBSCAN. KMeans clustering demonstrated the highest Calinski-Harabasz score (9.039247) and a silhouette score closer to 1 (0.004634), indicating effective clustering capability.\n","\n","Challenges encountered during model development included the difficulty in identifying the optimal number of clusters and the time-consuming nature of methods such as the Kelbow method and silhouette score for determining the ideal cluster count.\n","\n","In summary, the Netflix Movies and TV Shows Clustering project successfully categorized content into clusters based on their features, providing valuable insights into Netflix's content distribution and trends."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n","\n","In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n","\n","Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n","\n","In this project, you are required to do :\n","1. Exploratory Data Analysis\n","2. Understanding what type content is available in different countries\n","3. Is Netflix has increasingly focusing on TV rather than movies in recent years.\n","4. Clustering similar content by matching text-based features\n"],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Importing Numpy & Pandas for data processing & data wrangling\n","import numpy as np\n","import pandas as pd\n","\n","# Importing  tools for visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Importing libraries for hypothesis testing\n","from scipy.stats import uniform\n","from scipy.stats import norm\n","from scipy.stats import chi2\n","from scipy.stats import t\n","from scipy.stats import f\n","from scipy.stats import ttest_ind\n","import scipy.stats as stats\n","\n","# Word Cloud library\n","from wordcloud import WordCloud, STOPWORDS\n","\n","# Library used for textual data preprocessing\n","import string\n","string.punctuation\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","from nltk.stem.snowball import SnowballStemmer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n","from statsmodels.stats.proportion import proportions_ztest\n","\n","# Library used for Clusters implementation\n","from sklearn.cluster import KMeans\n","from yellowbrick.cluster import KElbowVisualizer\n","from sklearn.metrics import silhouette_score, silhouette_samples\n","from yellowbrick.cluster import SilhouetteVisualizer\n","from sklearn.cluster import AgglomerativeClustering\n","import scipy.cluster.hierarchy as sch\n","\n","# Library used for building recommendation system\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import *\n","\n","# Library used for ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","%matplotlib inline"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n","df.head()"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Last five observations\n","df.tail()"],"metadata":{"id":"6EfjDXvw1ghq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","df.shape\n","print(f'Dataset having {df.shape[0]} rows and {df.shape[1]} columns')"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking columns name of dataset\n","df.columns"],"metadata":{"id":"5yktGO9cBQTi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","df.info()"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","*   Movies are more widely available on Netflix than TV shows.\n","\n","*  Most of the movies and TV shows are produced in the United States.\n","\n","\n","*  Documentaries are available in the majority of movies and TV shows on Netflix.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"aFc4i2UrBh7h"}},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["#  Checking duplicate values\n","dup = df.duplicated().sum()\n","print(f'number of duplicated rows are {dup}')"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Observations :\n","1. dataset having 7787 rows and 12 columns\n","2. There are no duplicate values in this dataset.\n","3. Director, cast, country, date_added, and rating all have null values.\n","4. The feature release year is numerical, and everything else is categorical.\n","5. The date_added feature contains dates, but its datatype incorrectly associates an object.\n"],"metadata":{"id":"LM_JDSMBB6AH"}},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","count_null_df=pd.DataFrame({'columns':df.columns,'number_of_nulls_values':df.isna().sum()})\n","count_null_df.set_index('columns').sort_values(by='number_of_nulls_values', ascending = False)"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","plt.figure(figsize=(7,5))\n","sns.heatmap(df.isnull(), cbar=False)\n","plt.show()"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["Observations :\n","1. dataset having 7787 rows and 12 columns\n","2. There are no duplicate values in this dataset.\n","3. Director, cast, country, date_added, and rating all have null values.\n","4. The feature release year is numerical, and everything else is categorical.\n","5. The date_added feature contains dates, but its datatype incorrectly associates an object.\n"],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","columns=list(df.columns)\n","print(columns)"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","# Basic description of all Dataset\n","df.describe(include='all').round(2)"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finding Categorical variables\n","categorical_variables=[var for var in df.describe(include='object')]\n","print(f'Dataset having {len(categorical_variables)} categorical variables')\n","print('~~'*45)\n","print(categorical_variables)"],"metadata":{"id":"s43Ph9N8Ycti"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finding numerical variables\n","numerical_variables=[var for var in df.columns if var not in categorical_variables]\n","print(f'Dataset having {len(numerical_variables)} numerical variables')\n","print('~~'*45)\n","print(numerical_variables)"],"metadata":{"id":"-w5cF-ccYgqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["Observations :\n","1. The following features are included in the dataset:  show_id, type, title, director, cast, country, date_added, release_year, rating, duration, listed_in, and description.\n","2. Dataset having one numerical variable, release_year.\n","3. show_id, type, title, director, cast, country, date_added, rating, duration, listed_in, and description are the 11 categorical variables in this dataset.\n","\n","The dataset contains movies and tv shows information (show id, type, title, director, release year, rating, duration etc.).\n","Attribute Information :\n","\n","show_id: Unique Id number for all the listed rows\n","\n","type: denotes type of show namely TV Show or Movie\n","\n","title: title of the movie\n","\n","director: Name of director/directors\n","\n","cast: lists the cast of the movie\n","\n","country: country of the production house\n","\n","date_added: the date the show was added\n","\n","release_year: year of the release of the show\n","\n","rating: show ratings\n","\n","duration: duration of the show\n","\n","listed_in: the genre of the show\n","\n","description: summary/ description of the movie"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","for i in df.columns.tolist():\n","  print(\"No. of unique values in \",i,\"is\",df[i].nunique(),\".\")"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#total null values\n","df.isnull().sum().sum()"],"metadata":{"id":"uylgz96RAnOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Handling the missing values\n","df[['director','cast','country']] = df[['director','cast','country']].fillna('Unknown')\n","df['rating'] = df['rating'].fillna(df['rating'].mode()[0])\n","df.dropna(axis=0, inplace = True)"],"metadata":{"id":"hFfeKzZpAqmn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#again checking is there any null values are not\n","df.isnull().sum()"],"metadata":{"id":"tYXgtCkTAt8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","\n","# Before doing any data wrangling lets create copy of the dataset\n","data = df.copy()"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Filling cast null values as not available\n","data['cast'] = data['cast'].fillna(value='Not available')"],"metadata":{"id":"DQhA_SohBFqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Choosing the primary country and primary genre to simplify the analysis\n","data['country'] = data['country'].fillna(value='Not Known')\n","# data['country'] = data['country'].fillna(value=data['country'].mode())"],"metadata":{"id":"271pcFcCBOLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Since date_added and rating have low number of missing values, that are 10 and 7 respectively, i have dropping the same\n","data = data.dropna(subset=['date_added','rating'])"],"metadata":{"id":"TS807499BQ2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Checking missing values again for confirmation\n","data.isna().sum()"],"metadata":{"id":"EjiU0uwlBTmM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["To make the data analysis ready i have done the following:\n","\n","Filled missing values of cast with Not available.\n","Filled missing values of country with Not Known.\n","Filled missing values of director with Unknown.\n","Dropped rows of date_added missing values.\n","Dropped rows of ratings missing values.\n"],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Chart - 1 visualization code\n","\n","spread = data['type'].value_counts()\n","plt.rcParams['figure.figsize'] = (5,5)\n","\n","# Set Labels\n","spread.plot(kind = 'pie', autopct='%1.2f%%', cmap='Set1')\n","plt.title(f'Movie vs TV Show share')\n","plt.show()"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["A pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Wherever different percentage comparison comes into action, pie chart is used frequently. So, i have used Pie Chart and which helped us to get the percentage comparison more clearly and precisely."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["From the above chart, we got to know that the types of shows available in netflix is not even with high count for TV shows. 69.14% of the data belongs to movies and 30.86% of the data for TV shows.\n","\n","\n","\n"],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["The insights gained from the chart can potentially create a positive business impact by providing valuable information for decision-making. Understanding the distribution of categories in various columns helps identify patterns and target specific demographics or areas of focus. For example, businesses can develop tailored marketing campaigns based on the types of shows most watched by the audience."],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","\n","plt.figure(figsize=(10, 5))\n","sns.countplot(x='rating', hue='type', data=data, palette=['#564d4d', '#db0000'])\n","\n","# Set Labels\n","plt.title('Counts of Various Ratings')\n","plt.xlabel('Ratings')\n","plt.xticks(rotation = 60)\n","\n","# Display Chart\n","plt.show()\n","\n","# Printing The Counts of Each Rating for Different Type Shows\n","print('Each Rating Counts for Different Types of Shows:')\n","print(data.groupby(['rating', 'type']).size())"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["From above count plot we can clearly see that the most of the ratings are given by TV-MA followed by TV-14 and the least ratings are given by NC-17.\n","\n"],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["In summary, while insights gained from data analysis have the potential to drive positive business impact, it's crucial to assess their relevance, actionability, impact on KPIs, and sustainability. Additionally, businesses should be cautious of potential pitfalls and prioritize strategies that contribute to long-term growth and customer satisfaction."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 Count Plot Visualization Code for Content Produced by Different Countries\n","# Not Taking Unknown Countries\n","country_df = data[data['country'] != 'Not Known']\n","\n","# Set Labels\n","plt.figure(figsize=(10, 5))\n","sns.countplot(y='country', hue='type', data=country_df, palette=['#564d4d', '#db0000'], order=country_df.country.value_counts().iloc[:10].index)\n","plt.title('Top Ten Countries With Most Content')\n","plt.ylabel('Country')\n","\n","# Display Chart\n","plt.show()\n","\n","# Printing The Counts of Different Shows for Top 10 Countries\n","print('Number of Shows Produced by Top 10 Countries:')\n","print(country_df.groupby(['type']).country.value_counts().groupby(level=0, group_keys=False).head(10))"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["A bar plot shows catergorical data as rectangular bars with the height of bars proportional to the value they represent.\n","\n"],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["From above count plot we found that the content belongs to United States alone is 2546 (Movie: 1847, TV Show: 699) and followed by India is 923 (Movie: 852, TV Show: 71)."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["Yes, from above insight we got to know:\n","\n","The United States is a leading producer of both types of shows (Movies and TV Shows), this makes sense since Netflix is a US company.\n","\n","The influence of Bollywood in India explains the type of content available, and perhaps the main focus of this industry is Movies and not TV Shows.\n","\n","On the other hand, TV Shows are more frequent in South Korea, which explains the KDrama culture nowadays."],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","\n","# Create New DataFrames for Movie and TV Show Release\n","release_year_df = data[['type','release_year']]\n","movie_year = release_year_df[release_year_df['type']=='Movie'].release_year.value_counts().to_frame().reset_index().rename(columns={'index':'year','release_year':'count'})\n","\n","show_year = release_year_df[release_year_df['type']=='TV Show'].release_year.value_counts().to_frame().reset_index().rename(columns={'index':'year','release_year':'count'})"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(10, 5))\n","sns.lineplot(data=movie_year, x='year', y='count', color = '#db0000')\n","sns.lineplot(data=show_year, x='year', y='count', color = '#564d4d')\n","\n","# Set Labels\n","plt.title('Content Released Over The Years')\n","plt.legend(['Movie','TV Show'])\n","plt.xlabel('Release Year')\n","plt.ylabel('Count')\n","\n","# Display Chart\n","plt.show()"],"metadata":{"id":"HudRVMz6hc_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Printing The Counts of Different Shows Released for Top 10 Years\n","print('Number of Shows Released in Each Year:')\n","print(data.groupby(['type']).release_year.value_counts().groupby(level=0, group_keys=False).head(10))"],"metadata":{"id":"oFdx0LWZhgUx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["A line plot, also known as a line chart or line graph, is a way to visualize the trend of a single variable over time. It uses a series of data points connected by a line to show how the value of the variable changes over time.\n","\n","Line plots are useful because they can quickly and easily show trends and patterns in the data. They are particularly useful for showing how a variable changes over a period of time. They are also useful for comparing the trends of multiple variables.\n","\n","To see how the different contents are released over the years i have used line plot here."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["*From* above graph, it is observed that most of the content on netflix are of the release date from 2010 to 2020.\n","\n"],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"l_qAA5YmhsQp"}},{"cell_type":"markdown","source":["From the above insight we got to know:\n","\n","Growth in the number of movies on Netflix is much higher than tv shows.\n","Most of the content available was released between 2010 and 2020.\n","The highest number of movies got released in 2017 and 2018 and tv shows got released in 2019 and 2020.\n","The line plot shows very few movies, and tv shows got released before the year 2010 and in 2021. It is due to very little data collected from the year 2021."],"metadata":{"id":"0WVM4BHXhtZI"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["# Chart - 5 visualization code\n","# Length distribution of movies\n","# Extracting Month from date_added Column\n","data['month_added'] = pd.DatetimeIndex(data['date_added']).month"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create DataFrame To Store Month Values and Counts\n","months_df = data.month_added.value_counts().to_frame().reset_index().rename(columns={'index':'month', 'month_added':'count'})"],"metadata":{"id":"uSGepGXDh1ER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chart - 5 Count Plot Visualization Code for Month Wise Addition of Contents on Netflix\n","plt.figure(figsize=(10, 5))\n","ax=sns.barplot(data=months_df, x='month', y='count', palette='Reds_r')\n","\n","# Set Labels\n","plt.title('Month Wise Addition of Contents')\n","plt.xlabel('Month')\n","for i in ax.patches:\n","    ax.annotate(f'{i.get_height()}', (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 6), textcoords = 'offset points')\n","\n","# Display Chart\n","plt.show()"],"metadata":{"id":"Pq4OvO8Fh3f0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["From above graph, it is observed that most of the shows are uploaded either by year ending or beginning.n"],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"4-c3QQ4OiDTM"}},{"cell_type":"markdown","source":["From the above insight we got to know:\n","\n","October, November, December, and January are months in which many tv shows and movies get uploaded to the platform.\n","It might be due to the winter, as in these months people may stay at home and watch tv shows and movies in their free time."],"metadata":{"id":"4SaByHUciCU4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","# Splitting Movie and Separating Values\n","df_movies = data[data['type']=='Movie'].copy()\n","df_movies.duration = df_movies.duration.str.replace(' min','').astype(int)"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chart - 6 Histogram Visualization Code for Duration Distribution of Netflix Movies\n","plt.figure(figsize=(8,4), dpi=120)\n","sns.set(style=\"darkgrid\")\n","sns.histplot(df_movies.duration, color='#db0000')\n","plt.xticks(np.arange(0,360,30))\n","\n","# Set Labels\n","plt.title(\"Duration Distribution for Netflix Movies\")\n","plt.ylabel(\"% of All Netflix Movies\", fontsize=9)\n","plt.xlabel(\"Duration (minutes)\", fontsize=9)\n","\n","# Display Chart\n","plt.show()"],"metadata":{"id":"Xj5stEu4iUA-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["A histplot is a type of chart that displays the distribution of a dataset. It is a graphical representation of the data that shows how often each value or group of values occurs. Histplots are useful for understanding the distribution of a dataset and identifying patterns or trends in the data. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.\n","\n","Thus, I used the histogram plot to analysis the duration distributions for the netflix movies.\n","\n"],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["From the above chart we come to know that most of the movies last for 90 to 120 minutes.\n","\n"],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["From the above insight we got to know:\n","\n","On netflix most of the movies last for 90 to 120 minutes.\n","So for target audience, movies duration will be greater than minimum 90 minutes."],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","# Chart - 7 WordCloud Plot Visualization Code for Most Used Words in Netflix Shows Title\n","# Create a String to Store All The Words\n","comment_words = ''\n","\n","# Remove The Stopwords\n","stopwords = set(STOPWORDS)\n","\n","# Iterate Through The Column\n","for val in data.title:\n","\n","    # Typecaste Each Val to String\n","    val = str(val)\n","\n","    # Split The Value\n","    tokens = val.split()\n","\n","    # Converts Each Token into lowercase\n","    for i in range(len(tokens)):\n","        tokens[i] = tokens[i].lower()\n","\n","    comment_words += \" \".join(tokens)+\" \"\n","\n","# Set Parameters\n","wordcloud = WordCloud(width = 1000, height = 500,\n","                background_color ='white',\n","                stopwords = stopwords,\n","                min_font_size = 10,\n","                max_words = 1000,\n","                colormap = 'gist_heat_r').generate(comment_words)\n","\n","# Set Labels\n","plt.figure(figsize = (6,6), facecolor = None)\n","plt.title('Most Used Words In Shows Title', fontsize = 15, pad=20)\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.tight_layout(pad = 0)\n","\n","# Display Chart\n","plt.show()"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["The word cloud graphic is a visual representation that supplements a section of text to help readers better understand an idea or approach a subject from a different angle. A word cloud shows off trends."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["From above word cloud plot, it is observed that most repeated words in title include Christmas, Love, World, Man, and Story."],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["From the above insight we got to know:\n","\n","Most repeated words in title include Christmas, Love, World, Man, and Story.\n","We saw that most of the movies and tv shows got added during the winters, which tells why Christmas appeared many times in the titles."],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Assuming 'data' is your DataFrame containing the Netflix data\n","duplicate_directors = data[data.duplicated('director')]['director'].unique()\n","print(\"Duplicate Directors:\", duplicate_directors)\n"],"metadata":{"id":"FqoqLEkckkIv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_unique_directors = data.drop_duplicates('director')\n"],"metadata":{"id":"w2lJlg8okrx4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directors = data[data['director'] != 'Unknown']['director'].str.split(', ').explode()\n","\n","# Count the occurrences of each director\n","director_counts = directors.value_counts()\n","\n","# Select the top 10 directors\n","top_directors = director_counts.head(10)\n","\n","# Plotting the top 10 directors\n","plt.figure(figsize=(10,5))\n","sns.barplot(y=top_directors.index, x=top_directors.values, palette='Reds_r')\n","plt.title('Top 10 Directors on Netflix')\n","plt.xlabel('Number of Titles')\n","plt.ylabel('Director')\n","plt.show()"],"metadata":{"id":"0VW-CUpUkW-r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["From the above chart we come to know that the most popular director in netflix is Jan Sutar and followed by Raúl Campos and Marcus Raboy."],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["From the above insight we got to know:\n","\n","Jan Suter, Raúl Campos, Marcus Raboy, Jay Karas, Cathy Garcia-Molina, Jay Chapman are the top 5 directors which highest number of movies and tv shows are available in netflix.\n","As we stated previously regarding the top genres, it's no surprise that the most popular directors on Netflix with the most titles are mainly international as well."],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["# Chart - 9 visualization code\n","\n","\n","\n","genres = df['listed_in'].str.split(', ').explode()\n","genre_counts = genres.value_counts()\n","\n","# Select the top 10 genres\n","top_genres = genre_counts.head(10)\n","\n","# Plotting the top 10 genres\n","plt.figure(figsize=(10,5))\n","sns.barplot(y=top_genres.index, x=top_genres.values, palette='Reds_r')\n","plt.title('Top 10 Genres on Netflix')\n","plt.xlabel('Number of Titles')\n","plt.ylabel('Genre')\n","plt.show()"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":[" A bar graph is a graphical representation of data in which we can highlight the category with particular shapes like a rectangle. The length and heights of the bar chart represent the data distributed in the dataset. In a bar chart, we have one axis representing a particular category of a column in the dataset and another axis representing the values or counts associated with it. Bar charts can be plotted vertically or horizontally. A vertical bar chart is often called a column chart."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["From above graph, it is observed that international movies is in top in terms of genre and followed by dramas and comedies."],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"FdL7BeO-mLa7"}},{"cell_type":"markdown","source":["From the above insight we got to know:\n","\n","In terms of genres, international movies takes the cake surprisingly followed by dramas and comedies.\n","Even though the United States has the most content available, it looks like Netflix has decided to release a ton of international movies."],"metadata":{"id":"e6b1v-EqmKL3"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["# Chart - 10 visualization code\n","\n","\n","actor = data[data['cast'] != 'Not available']['cast'].str.split(', ').explode()\n","\n","# Count the occurrences of each actor\n","actor_counts = actor.value_counts()\n","\n","# Select the top 10 actors\n","top_actors = actor_counts.head(10)\n","\n","# Plotting the top 10 actors\n","plt.figure(figsize=(10,5))\n","sns.barplot(y=top_actors.index, x=top_actors.values, palette='Reds_r')\n","plt.title('Top 10 Actors on Netflix')\n","plt.xlabel('Number of Titles')\n","plt.ylabel('Actor')\n","plt.show()"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["From above graph, it is observed that most popular actors with most content in netflix are Anupam Kher, Shah Rukh Khan, Naseeruddin Shah and followed by Om Puri and Takahiro Sakurai."],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["From the above insight we got to know:\n","\n","That the actors in the top ten list of most numbers tv shows and movies are from India.\n","Anupam Kher and Shah Rukh Khan have 30 above content alone in netflix."],"metadata":{"id":"R4Ka1PC2phqR"}},{"cell_type":"markdown","source":["#### Chart - 11"],"metadata":{"id":"x-EpHcCOp1ci"}},{"cell_type":"code","source":["# Chart - 11 visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming df is your DataFrame containing 'release_year' and 'type' columns\n","plt.figure(figsize=(10, 5))\n","p = sns.countplot(x='release_year', data=df, hue='type')\n","plt.title('Number of movies and TV shows added over the years')\n","plt.xlabel('Release Year')\n","\n","for i in p.patches:\n","    p.annotate(format(i.get_height(), '.0f'),\n","               (i.get_x() + i.get_width() / 2., i.get_height()),\n","               ha='center', va='center', xytext=(0, 10),\n","               textcoords='offset points')\n","\n","plt.show()\n","\n"],"metadata":{"id":"mAQTIvtqp1cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"X_VqEhTip1ck"}},{"cell_type":"markdown","source":["The correlation coefficient is a measure of the strength and direction of a linear relationship between two variables. A correlation matrix is used to summarize the relationships among a set of variables and is an important tool for data exploration and for selecting which variables to include in a model. The range of correlation is [-1,1].\n","\n","Thus to know the correlation between all the variables along with the correlation coeficients, we have used correlation heatmap."],"metadata":{"id":"-vsMzt_np1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"8zGJKyg5p1ck"}},{"cell_type":"markdown","source":["Since there is only one value in dataframe of int type, we are unable to visualize the Correlation Matrix heatmap."],"metadata":{"id":"ZYdMsrqVp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"PVzmfK_Ep1ck"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"druuKYZpp1ck"}},{"cell_type":"markdown","source":["#### Chart - 12"],"metadata":{"id":"n3dbpmDWp1ck"}},{"cell_type":"code","source":["# Chart - 12 visualization code\n","# Number of shows released each year since 2008\n","order = range(2008,2022)\n","plt.figure(figsize=(10,5))\n","p = sns.countplot(x='release_year',data=df, hue='type',\n","                  order = order)\n","plt.title('Number of shows released each year since 2008 that are on Netflix')\n","plt.xlabel('')\n","for i in p.patches:\n","  p.annotate(format(i.get_height(), '.0f'), (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n"],"metadata":{"id":"bwevp1tKp1ck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"ylSl6qgtp1ck"}},{"cell_type":"markdown","source":["A bivariate plot graphs the relationship between two variables that have been measured on a single sample of subjects. Such a plot permits you to see at a glance the degree and pattern of relation between the two variables."],"metadata":{"id":"m2xqNkiQp1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ZWILFDl5p1ck"}},{"cell_type":"markdown","source":["in this graph we found that Number of shows released each year since 2008 that are on Netflix.see this graph Before 2019 Movies are highest number of released but 2020 and 2021 TV shows are the highest number of released.\n","\n"],"metadata":{"id":"x-lUsV2mp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"M7G43BXep1ck"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"5wwDJXsLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 13"],"metadata":{"id":"Ag9LCva-p1cl"}},{"cell_type":"code","source":["# Chart - 13 visualization code\n","# Seasons in each TV show\n","plt.figure(figsize=(10,5))\n","p = sns.countplot(x='duration',data=df[df['type']=='TV Show'])\n","plt.title('Number of seasons per TV show distribution')\n","\n","for i in p.patches:\n","  p.annotate(format(i.get_height(), '.0f'), (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n"],"metadata":{"id":"EUfxeq9-p1cl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"E6MkPsBcp1cl"}},{"cell_type":"markdown","source":["A count plot can be thought of as a histogram across a categorical, instead of quantitative, variable. The basic API and options are identical to those for barplot() , so you can compare counts across nested variables."],"metadata":{"id":"V22bRsFWp1cl"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"2cELzS2fp1cl"}},{"cell_type":"markdown","source":[" In this graph we found Number of per Seasons in each TV show count.first seasons are 1608 and second seasons are 378 and third seasons are 183 number of counting are there."],"metadata":{"id":"ozQPc2_Ip1cl"}},{"cell_type":"markdown","source":["#### Chart - 14 - Correlation Heatmap"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["# Correlation Heatmap visualization code\n","corr_matrix = data.corr()\n","\n","# Plot Heatmap\n","plt.figure(figsize=(14,7))\n","sns.heatmap(corr_matrix, annot=True, cmap='Reds_r')\n","\n","# Setting Labels\n","plt.title('Correlation Matrix heatmap')\n","\n","# Display Chart\n","plt.show()"],"metadata":{"id":"xyC9zolEZNRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["i. A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1]."],"metadata":{"id":"DVPuT8LYZNRQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"YPEH6qLeZNRQ"}},{"cell_type":"markdown","source":["ii. Thus to know the correlation between all the variables along with the correlation coefficients, i used correlation heatmap.\n","\n","iii. In this correlation Heatmap graph we found the Target ages proportion of total content by country."],"metadata":{"id":"bfSqtnDqZNRR"}},{"cell_type":"markdown","source":["#### Chart - 15 - Pair Plot"],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["# Pair Plot visualization code\n","sns.pairplot(df, palette=\"husl\")\n","plt.show()"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"EXh0U9oCveiU"}},{"cell_type":"markdown","source":["1. Based on the plot of release_year and year_added, we can conclude that Netflix is increasingly adding and releasing movies and TV shows over time.\n","2. We can conclude from plot release_year and month_added that Netflix releases movies and TV shows throughout the all months of the year."],"metadata":{"id":"eMmPjTByveiU"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"22aHeOlLveiV"}},{"cell_type":"markdown","source":["Observations :\n","1. We see that the movie or TV show release year and day of the month on movies or TV shows added to Netflix are slightly correlated with each other.\n","2. Based on the plot of release_year and year_added, we can conclude that Netflix is increasingly adding and releasing movies and TV shows over time.\n","2. We can conclude from plot release_year and month_added that Netflix releases movies and TV shows throughout the all months of the year."],"metadata":{"id":"uPQ8RGwHveiV"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["Based on above chart experiments i have noticed that some variable of our netflix dataset does not seems to normally distributed so i have made hypothetical assumption that our data is normally distributed and for that i have decided to do statistical analysis.\n","\n","Average number of movies on Netflix in United States is greater than the average number of movies on Netflix in India.\n","The number of movies available on Netflix is greater than the number of TV shows available on Netflix."],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["Average number of movies on Netflix in United States is greater than the average number of movies on Netflix in India."],"metadata":{"id":"4eZGRcjNo7MJ"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["Null hypothesis:  Ho:μunitedstates=μindia\n","\n","Alternate hypothesis:  H1:μunitedstates≠μindia\n","\n","Test Type: Two-sample t-test"],"metadata":{"id":"SpddAxUzpCYp"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Split the data into the 'united states' and 'india's' movie produced groups\n","us_movie_df = df_movies[df_movies.country == 'United States']\n","india_movie_df = df_movies[df_movies.country == 'India']"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform the two-sample t-test between the release years of the two groups of movies\n","import scipy\n","t_stat, p_val = scipy.stats.ttest_ind(us_movie_df['release_year'], india_movie_df['release_year'], equal_var=False)\n","\n","# Print the results\n","if p_val < 0.05:\n","    print(f\"Since p-value ({p_val}) is less than 0.05, we reject null hypothesis.\\nHence, There is a significant difference in average number of movies produced by the 'United States' and 'India'.\")\n","else:\n","  print(f\"Since p-value ({p_val}) is greater than 0.05, we fail to reject null hypothesis.\\nHence, There is no significant difference in average number of movies produced by the 'United States' and 'India'.\")"],"metadata":{"id":"6l3QjrbOpTAi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["I selected the two-sample t-test for this analysis as it is suitable for comparing the means of two independent samples. In this case, we have two separate sets of movies data from Netflix for the United States and India, and we aim to determine if there is a significant difference in the average number of movies between these two countries."],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["The number of movies available on Netflix is greater than the number of TV shows available on Netflix."],"metadata":{"id":"oaiJslETpeZv"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["Null hypothesis:  Ho:μmovie=μtvshow\n","\n","Alternate hypothesis:  H1:μmovie≠μtvshow\n","\n","Test Type: Two sample z-test"],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Count the number of movies and TV shows in the DataFrame\n","n_movies = data[data['type'] == 'Movie'].count()['type']\n","n_tv_shows = data[data['type'] == 'TV Show'].count()['type']"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the counts and sample sizes for the z-test\n","counts = [n_movies, n_tv_shows]  # Number of movies and TV shows\n","nobs = [len(data), len(data)]  # Total number of observations in the DataFrame\n","\n","# Perform a two sample z-test assuming equal proportions\n","z_stat, p_val = proportions_ztest(counts, nobs, value=0, alternative='larger')\n","\n","# Print the results\n","print('Z-statistic:', z_stat)\n","print('P-value:', p_val)\n","print()\n","\n","if p_val < 0.05:\n","    print(f\"Since p-value ({p_val}) is less than 0.05, we reject null hypothesis.\\nHence, There is a significant difference in number of 'movies' and 'TV shows' available on Netflix.\")\n","else:\n","  print(f\"Since p-value ({p_val}) is greater than 0.05, we fail to reject null hypothesis.\\nHence, There is no significant difference in number of 'movies' and 'TV shows' available on Netflix.\")"],"metadata":{"id":"W7xzTQDDpptV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["To compare the number of movies and TV shows available on Netflix, I conducted a two-sample z-test for proportions to obtain the p-value."],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["I choose the two-sample z-test for proportions to compare the number of movies and TV shows available on Netflix because the data consists of two categorical variables."],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","from scipy.stats import ttest_ind\n","\n","\n","# Create a new column indicating the presence of well-known actors\n","data['well_known_actor'] = data['cast'].apply(lambda x: 1 if 'Tom Cruise' in x else 0)  # Adjust 'Tom Cruise' with the actor name you're interested in\n","\n","# Separate data into two groups: with and without well-known actors\n","with_actors = data[data['well_known_actor'] == 1]['release_year']\n","without_actors = data[data['well_known_actor'] == 0]['release_year']\n","\n","# Perform two-sample t-test\n","t_statistic, p_value = ttest_ind(with_actors, without_actors, equal_var=False)\n","\n","print(\"T-statistic:\", t_statistic)\n","print(\"P-value:\", p_value)"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["I performed a chi-square test of independence to obtain the p-value."],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["I chose the chi-square test of independence because it is appropriate for analyzing the relationship between two categorical variables, such as the presence of well-known actors (binary variable) and the genre of Netflix content (multicategorical variable). This test allows us to determine if there is a significant association between these variables.\n","\n","\n","\n"],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Checking duplicated values in dataset\n","data.isna().sum().sum()"],"metadata":{"id":"4sOvyWNLZnZS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n","# Checking for number of null values\n","null_df=pd.DataFrame({'columns':df.columns,'number_of_nulls_values':df.isna().sum(),'percentage_null_values':round(df.isna().sum()*100/len(df),2)})\n","null_df.set_index('columns').sort_values(by='percentage_null_values', ascending = False)"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ploting number of null values with its variable\n","plt.figure(figsize=(10,7))\n","ax=sns.barplot(x='columns', y='percentage_null_values', data=null_df)\n","ax.bar_label(ax.containers[0])\n","plt.title('Percentage of null values with respect to Features')\n","plt.xticks(rotation= 45)\n","plt.show()"],"metadata":{"id":"10MNcB1gZ4sP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["Director, cast, country, date_added, and rating have null values in 30.68%, 9.22%, 6.51%, 0.13%, and 0.09% of their respective features."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"code","source":["# Vizualizing the null values from the dataset\n","plt.figure(figsize=(17,5))\n","sns.heatmap(df.isnull(),cbar=True, cmap='BuPu')\n","plt.title('Missing values in the dataset with respect to its features')"],"metadata":{"id":"X96dZ4IKaSsb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#checking category of features whoes having null values\n","\n","# Defining target variables\n","null_variables=['director','cast','country','date_added','rating']\n","# Checking categories in each target feature\n","for var in null_variables:\n","    print(df[var].value_counts())\n","    print('~~'*45)"],"metadata":{"id":"tvkuZYLTaWK2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Handling null values\n","\n","# Filling null values of features director, cast and country\n","df.director.fillna(\"Director Unavailable\",inplace=True)\n","df.cast.fillna(\"Cast Unavailable\",inplace=True)\n","df.country.fillna(\"Country Unavailable\",inplace=True)\n","\n","# Dropping null values from date_added and rating columns\n","df.dropna(subset=[\"date_added\",'rating'],inplace=True)"],"metadata":{"id":"dTVslDwYaaVK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Since there are many null values for features like director, cast, and country, those null values cannot be dropped; instead, they have been substituted with director Unavailable, Cast Unavailability, and Country Unavailable, accordingly.\n","2. Features such as date_added and rating have a very low number of null values, so we dropped those null values."],"metadata":{"id":"kxuaBqhOafty"}},{"cell_type":"code","source":["# Check for null values that were eliminated or were not successfully eliminated.\n","df.isnull().sum()"],"metadata":{"id":"j0rOVxRSao0C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Nullfree shape of dataset\n","df.shape"],"metadata":{"id":"dMzIfMpFavpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["# Handling Outliers & Outlier treatments\n","## Checking for outliers in numerical variables using boxplot\n","\n","# Importing library for checking normality in distribution\n","from scipy.stats import norm\n","\n","# Plotting Box and Distribution plot using for loop\n","for var in numerical_variables:\n","    plt.figure(figsize=(15,6))\n","    plt.subplot(1,2,1)\n","    ax=sns.boxplot(data=df[var])\n","    ax.set_title(f'{var}')\n","    ax.set_ylabel(var)\n","\n","    plt.subplot(1,2,2)\n","    ax=sns.distplot(df[var], fit=norm)\n","    ax.set_title(f'skewness of {var} : {df[var].skew()}')\n","    ax.set_xlabel(var)\n","    print('__'*45)\n","    plt.show()\n","    print('__'*45)"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Variable release_year have outliers.\n"],"metadata":{"id":"ZuQJLGTla6EB"}},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["Regarding outlier treatment techniques, the code provided focuses on visual identification of outliers using boxplots and examining skewness. However, the code snippet doesn't explicitly perform any outlier treatment. Outlier treatment techniques could include:"],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["# Expand Contraction\n","## Creating new feature having length of words of description variable\n","# df['description_len'] = df['description'].apply(lambda x: len(x.split()))\n","# df.head()\n","\n","data['tags'] = data['description'] + ' ' + data['rating'] + ' ' + data['country'] + ' ' + data['listed_in'] + ' ' + data['cast']"],"metadata":{"id":"PTouz10C3oNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Install contraction library in the envirnoment\n","# !pip install contractions\n","# Cross checking our result for the function created\n","print(data['tags'][0])"],"metadata":{"id":"jKnHN0VycEbE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Lower Casing\n","# # Updating nltk library\n","# import nltk\n","# nltk.download('stopwords')\n","\n","# Define a function to convert text into lower cases\n","def to_lower(x):\n","  return x.lower()\n","\n","# Apply the to_lower() function to the 'tags' column of the DataFrame\n","data['tags'] = data['tags'].apply(to_lower)\n","\n","# Cross checking our result for the function created\n","print(data['tags'][0])"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["# Remove Punctuations\n","def remove_punctuation(text):\n","    '''a function for removing punctuation'''\n","    # Replace each punctuation mark with no space, effectively deleting it from the text\n","    translator = str.maketrans('', '', string.punctuation)\n","    text_without_punct = text.translate(translator)\n","    return text_without_punct\n","\n","# Apply the remove_punctuation function to the 'tags' column of the DataFrame\n","data['tags'] = data['tags'].apply(remove_punctuation)\n","\n","# Cross-check our result that the function worked as expected\n","print(data['tags'][0])\n"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","*   The majority of the corpus contains punctuation that accounts for less than 5% of the total corpus.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"pEAvITIWc2ao"}},{"cell_type":"markdown","source":["Successfully converted to lower case and removed stopwords and punctuation from the corpus using function remove_stopwords_punctuations.\n"],"metadata":{"id":"VoyXDeisdFJ4"}},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["# Remove URLs & Remove words and digits contain\n","# 'tags' column does not have any URLs so remove words and digits containing digits\n","data['tags'] = data['tags'].str.replace(r'\\w*\\d\\w*', '', regex=True)\n","\n","# Cross-check our result for the function created\n","print(data['tags'][0])"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All URLs are removed from each corpus from variable no_sw_pun_description.\n"],"metadata":{"id":"AbZhwf9kdPWf"}},{"cell_type":"markdown","source":["5. . Removing Stopwords & Removing White spaces"],"metadata":{"id":"X4g96cE-yiHG"}},{"cell_type":"code","source":["# Since the language is english, we need to import english stop words\n","stop_words = nltk.corpus.stopwords.words('english')\n","\n","def remove_stop_words(x):\n","  ''' function to remove stop words'''\n","  x = x.split()\n","  res = ''\n","  for word in x:\n","    if word not in stop_words:\n","      res = res + ' ' + word\n","  return res\n","\n","# Apply the remove_stop_words function to the 'tags' column of the DataFrame\n","data['tags'] = data['tags'].apply(remove_stop_words)\n","\n","# Cross-check our result for the function created\n","print(data['tags'][0])"],"metadata":{"id":"veZCbvHCyea-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove White spaces in 'tags' column\n","data['tags'] = data['tags'].str.strip()\n","\n","# Cross-check our result for the function created\n","print(data['tags'][0])"],"metadata":{"id":"yCqR2N9Cypz1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Rephrase Text"],"metadata":{"id":"c49ITxTc407N"}},{"cell_type":"code","source":["# Rephrase Text\n","!pip install transformers\n"],"metadata":{"id":"foqY80Qu48N2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Initialize the paraphrasing pipeline\n","paraphrase_pipeline = pipeline(\"text2text-generation\", model=\"tuner007/pegasus_paraphrase\")\n","\n","# Text to be paraphrased\n","text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Generate paraphrases\n","paraphrases = paraphrase_pipeline(text, max_length=50, top_k=5)\n","\n","# Print the paraphrases\n","for i, paraphrase in enumerate(paraphrases):\n","    print(f\"Paraphrase {i+1}: {paraphrase['generated_text']}\")"],"metadata":{"id":"4popQgiXvRKl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 7. Tokenization"],"metadata":{"id":"OeJFEK0N496M"}},{"cell_type":"code","source":["# Assuming 'df' is your DataFrame containing the data\n","import nltk\n","nltk.download('punkt')\n","\n","# Apply the tokenization to the 'tags' column of the DataFrame\n","data['tags'] = data['tags'].apply(nltk.word_tokenize)\n","\n","# Cross-check our result that the function worked as expected\n","print(data['tags'][0])\n","\n","# Store this list form of 'tags' column as 'temp_tags' for later POS tagging purpose\n","temp_tags = data['tags']\n"],"metadata":{"id":"0xiLsVvJutH3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8. Text Normalization"],"metadata":{"id":"9ExmJH0g5HBk"}},{"cell_type":"code","source":["# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n","# Create an object of stemming function\n","stemmer = SnowballStemmer(\"english\")\n","\n","# Define a function to Normalize Text function\n","def stemming(text):\n","    '''a function which stems each word in the given text'''\n","    text = [stemmer.stem(word) for word in text]\n","    return \" \".join(text)\n","\n","# Apply the stemming function to the 'tags' column of the DataFrame\n","data['tags'] = data['tags'].apply(stemming)\n","\n","# Cross-check our result for the function created\n","print(data['tags'][0])"],"metadata":{"id":"AIJ1a-Zc5PY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text normalization technique have you used and why?"],"metadata":{"id":"cJNqERVU536h"}},{"cell_type":"markdown","source":["Stemming Stemming is a process to reduce the word to its root stem for example run, running, runs, runed derived from the same word as run. basically stemming do is remove the prefix or suffix from word like ing, s, es, etc. NLTK library is used to stem the words. There are various types of stemming algorithms like porter stemmer, snowball stemmer. Porter stemmer is widely used present in the NLTK library.\n","\n","The stemming technique is not used for production purposes because it is not so efficient technique and most of the time it stems the unwanted words. So, to solve the problem another technique came into the market as Lemmatization.\n","\n","So we use lemmitization, not stemming, for text normalization here.\n","\n","We write the raw function, which stems the word using Porter Stemmer, but we will use only lemmatization for text normalization, so the stemming function is not to be executed."],"metadata":{"id":"Z9jKVxE06BC1"}},{"cell_type":"markdown","source":["#### 9. Part of speech tagging"],"metadata":{"id":"k5UmGsbsOxih"}},{"cell_type":"code","source":["# POS Taging\n","# Loading Libraries\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Apply the pos tagging to the 'tags' column of the DataFrame\n","data['tags'] = temp_tags.apply(nltk.pos_tag)\n","\n","# Cross-check our result for the function created\n","print(data['tags'][0])\n"],"metadata":{"id":"btT3ZJBAO6Ik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('tagsets')\n","nltk.help.upenn_tagset()"],"metadata":{"id":"KxJSc0jTzF_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a function which gives true word (appropriate word) after pos tagging\n","def sentence(data):\n","  x=\"\"\n","  for i in data:\n","    a=i[0]+' '\n","    x=x+a\n","  return x\n","\n","# Apply the sentence function to the 'tags' column of the DataFrame\n","data['tags']=data['tags'].apply(sentence)\n","\n","# Cross-check our result for the function created\n","print(data['tags'][0])"],"metadata":{"id":"P6sJyFcrzSKB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10. Text Vectorization"],"metadata":{"id":"T0VqWOYE6DLQ"}},{"cell_type":"code","source":["# Create the object of tfid vectorizer\n","tfidf = TfidfVectorizer(stop_words='english', lowercase=False, max_features = 9000)\n","# setting max features = 9000 to prevent system from crashing\n","\n","# Fit the vectorizer using the text data\n","tfidf.fit(data['tags'])\n","\n","# Collect the vocabulary items used in the vectorizer\n","dictionary = tfidf.vocabulary_.items()"],"metadata":{"id":"yBRtdhth6JDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert vector into array form for clustering\n","vector = tfidf.transform(data['tags']).toarray()\n","\n","# Summarize encoded vector\n","print(vector)"],"metadata":{"id":"zRtEjJklzinI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tfidf.get_feature_names_out())"],"metadata":{"id":"Di2gIemazmFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vec_data=pd.DataFrame(vector)\n","vec_data"],"metadata":{"id":"bBjz6oojzpAL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text vectorization technique have you used and why?"],"metadata":{"id":"qBMux9mC6MCf"}},{"cell_type":"markdown","source":["I have use TF-IDF techique for vectorization.\n","\n","TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents (also known as a corpus).\n","\n","I have use TF-IDF because TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words. I can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions."],"metadata":{"id":"su2EnbCh6UKQ"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["\n","import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","# Load the Iris dataset as an example\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Create a DataFrame from the Iris dataset\n","feature_names = iris.feature_names\n","df = pd.DataFrame(X, columns=feature_names)\n","\n","# Step 1: Feature Scaling (Standardization)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","df_scaled = pd.DataFrame(X_scaled, columns=feature_names)\n","\n","# Step 2: Feature Transformation (Principal Component Analysis - PCA)\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","df_pca = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2'])\n","\n","# Step 3: Feature Engineering (Creating new features)\n","# For example, you can create interaction terms or polynomial features\n","df['sepal_length_width_ratio'] = df['sepal length (cm)'] / df['sepal width (cm)']\n","df['petal_length_width_ratio'] = df['petal length (cm)'] / df['petal width (cm)']\n","\n","# You can further manipulate features to minimize feature correlation or engineer new features based on domain knowledge\n","\n","# Concatenate the new features with the PCA-transformed features\n","df_final = pd.concat([df_pca, df[['sepal_length_width_ratio', 'petal_length_width_ratio']]], axis=1)\n","\n","# Check the correlation matrix to ensure reduced correlation among features\n","correlation_matrix = df_final.corr()\n","print(\"Correlation Matrix:\")\n","print(correlation_matrix)\n"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["# X = df.drop(columns=['show_id', 'title', 'description'])\n","\n","# # Convert categorical variables into numerical using one-hot encoding if needed\n","# # For example, if 'type', 'director', 'cast', 'country', 'rating', 'listed_in' are categorical:\n","# X = pd.get_dummies(X)\n","\n","# # Perform feature selection using SelectKBest with chi-squared test\n","# selector = SelectKBest(score_func=chi2, k=10)  # Select 10 best features\n","# X_selected = selector.fit_transform(X, y)\n","\n","# # Get the selected feature names\n","# selected_features = X.columns[selector.get_support()]\n","\n","# # Print the selected feature names\n","# print(\"Selected features:\", selected_features)"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*italicized text*##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"code","source":["# Transform Your data\n","#no i dont think my data need transformation"],"metadata":{"id":"I6quWQ1T9rtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["# Scaling your data"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?"],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["Yes its needed, because dimensionality reduction removes the least important variables from the model. That will reduce the model's complexity and also remove some noise in the data. Its also helps to mitigate overfitting."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"code","source":["# DImensionality Reduction (If needed)\n","from sklearn.decomposition import PCA\n","\n","# Initialize PCA object with a specific random state\n","pca = PCA(random_state=32)\n","\n","# Fit the PCA model to your data\n","pca.fit(vector)\n"],"metadata":{"id":"kQfvxBBHDvCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot a Graph for PCA\n","plt.figure(figsize=(12, 5))\n","plt.plot(np.cumsum(pca.explained_variance_ratio_))\n","\n","# Set labels\n","plt.title('PCA - cumulative explained variance vs number of components')\n","plt.xlabel('Number of components')\n","plt.ylabel('Cumulative explained variance')\n","plt.axhline(y= 0.8, color='red', linestyle='--')\n","plt.axvline(x= 2500, color='green', linestyle='--')\n","\n","# Display chart\n","plt.show()"],"metadata":{"id":"VmVnLS9v6CBY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reducing the dimensions to 2500 using pca\n","pca = PCA(n_components=2500, random_state=32)\n","pca.fit(vector)"],"metadata":{"id":"i0m6yCGM6DFt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transformed features\n","X = pca.transform(vector)"],"metadata":{"id":"9DyuZsMt6HmV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"],"metadata":{"id":"T5CmagL3EC8N"}},{"cell_type":"markdown","source":["We can use PCA to reduce the dimensionality of data.\n","\n","Because of the versatility and interpretability of PCA, it has been shown to be effective in a wide variety of contexts and disciplines. Given any high-dimensional dataset, we can start with PCA in order to visualize the relationship between points, to understand the main variance in the data, and to understand the intrinsic dimensionality.\n","\n","Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data."],"metadata":{"id":"ZKr75IDuEM7t"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely."],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["# Handling Imbalanced Dataset (If needed)\n","\n","# no handling Imbalanced Dataset needed"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["# ML Model - 1 Implementation\n","\n","model = KMeans()\n","visualizer = KElbowVisualizer(model, k=(3,12), metric='distortion', timings=False, locate_elbow=False)\n","\n","# Fit the data to the visualizer\n","visualizer.fit(X)"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From this plot, we can say that the best k value is 6. Because, after this point the distortion/inertia is start decreasing in a linear fashion."],"metadata":{"id":"sc8sAcMA6pbH"}},{"cell_type":"code","source":["# Instantiate the K-Means clustering model where number of clusters is 6\n","kmean=KMeans(n_clusters=6)\n","\n","# Fit the data to the KMean cluster\n","kmean.fit(X)\n","\n","# Predict on the model\n","y_kmean=kmean.predict(X)"],"metadata":{"id":"vMGfNx_I6s-z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Labelling the dataset as per the cluster."],"metadata":{"id":"8q1W2Xvp6xGc"}},{"cell_type":"code","source":["# Adding a new column 'K_mean_cluster' in the dataset\n","data[\"K_mean_cluster\"]=y_kmean\n","data.head()"],"metadata":{"id":"JKu1JdBW6ykL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting unique labels\n","u_labels = np.unique(y_kmean)\n","\n","# Plotting the results:\n","plt.figure(figsize=(10,5))\n","for i in u_labels:\n","    plt.scatter(X[y_kmean == i , 0] ,X[y_kmean == i , 1] , label = i)\n","plt.title('Clusters for K-Means Clustering')\n","plt.legend()\n","\n","# Display Chart\n","plt.show()"],"metadata":{"id":"k3m4T5YL7J_p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Second chart (Scatter plot Graph) shows the result of our algorithm. We can see that cluster wise data distribution in this chart"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here I used K-Means Clustering.\n","\n","K means number of clusters.\n","\n","K-means is a centroid-based clustering algorithm, where we calculate the distance between each data point and a centroid to assign it to a cluster. The goal is to identify the K number of groups in the dataset.\n","\n","K-means clustering distinguishes itself from Hierarchical since it creates K random centroids scattered throughout the data. The algorithm looks a little bit like…\n","\n","(1) Initialize K random centroids.\n","\n","You could pick K random data points and make those your starting points.\n","\n","Otherwise, you pick K random values for each variable.\n","\n","(2) For every data point, look at which centroid is nearest to it.\n","\n","Using some sort of measurement like Euclidean or Cosine distance.\n","\n","(3) Assign the data point to the nearest centroid.\n","\n","(4) For every centroid, move the centroid to the average of the points assigned to that centroid.\n","\n","(5) Repeat the last three steps until the centroid assignment no longer changes.\n","\n","The algorithm is said to have “converged” once there are no more changes.\n","\n","These centroids act as the average representation of the points that are assigned to it. This gives you a story almost right away. You can compare the centroid values and tell if one cluster favors a group of variables or if the clusters have logical groupings of key variables."],"metadata":{"id":"4hl5mc6n7TK0"}},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# First chart gives the optimal number of clusters. We get this chart by validation and basis of some rules"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["Here i have use Elbow Method for optimal number of k.\n","\n","The elbow method is a graphical representation of finding the optimal 'K' in a K-means clustering. It works by finding WCSS (Within-Cluster Sum of Square). i.e. the sum of the square distance between points in a cluster and the cluster centroid.\n","\n","In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters and picking the elbow of the curve as the number of clusters to use."],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"markdown","source":["Here we can not directly predict the number of cluster. After using Elbow method we can get optimal number of clusters and we can implement it directly."],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","plt.figure(figsize=(13,6))\n","dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n","\n","# Set labels\n","plt.title('Dendrogram')\n","plt.ylabel('Euclidean Distances')\n","plt.axhline(y=5.8, color='r', linestyle='--')\n","\n","# Display Chart\n","plt.show()"],"metadata":{"id":"yEl-hgQWpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"ph4HeY9B7qc0"}},{"cell_type":"markdown","metadata":{"id":"Gw8JLj9hKqwo"},"source":["From this graph we can say that optimal number of clusters is 6."]},{"cell_type":"code","source":["# Instantiate the Agglomerative clustering model where number of clusters is 6\n","aggh = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')\n","\n","# Fit the data to the Agglomerative cluster\n","aggh.fit(X)\n","\n","# Predict on the model\n","y_hc=aggh.fit_predict(X)"],"metadata":{"id":"b0FfSW9j7vBp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adding a new column 'Agg_cluster' in the dataset\n","data[\"Agg_cluster\"]=y_hc\n","data.head()"],"metadata":{"id":"xQBBzDER7xI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting unique labels\n","u_labels = np.unique(y_hc)\n","\n","# Plotting the results:\n","plt.figure(figsize=(10,5))\n","for i in u_labels:\n","    plt.scatter(X[y_hc == i , 0] ,X[y_hc == i , 1] , label = i)\n","plt.title('Clusters for Agglomerative Clustering')\n","plt.legend()\n","\n","# Display Chart\n","plt.show()"],"metadata":{"id":"tcwT25if7z9U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"],"metadata":{"id":"Dn0EOfS6psJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["### ML Model - 3"],"metadata":{"id":"Fze-IPXLpx6K"}},{"cell_type":"code","source":["# ML Model - 3 Implementation\n","\n","# Finding optimal number of clusters using the Silhouette Score\n","for n_clusters in range(2,15):\n","  km = KMeans (n_clusters=n_clusters, init ='k-means++', random_state=51)\n","  km.fit(X)\n","  preds = km.predict(X)\n","  centers = km.cluster_centers_\n","  score = silhouette_score(X, preds, metric='euclidean')\n","  print (\"For n_clusters = %d, silhouette score is %0.4f\"%(n_clusters, score))"],"metadata":{"id":"FFrSXAtrpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate Silhouette Plots for Each Clusters\n","# Instantiate the clustering model and visualizer\n","for n_clusters in range(2,15):\n","    km = KMeans (n_clusters=n_clusters, init ='k-means++', random_state=51)\n","    km.fit(X)\n","    preds = km.predict(X)\n","    centers = km.cluster_centers_\n","\n","    # Set parameters and labels\n","    score = silhouette_score(X, preds, metric='euclidean')\n","    print (\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))\n","\n","    visualizer = SilhouetteVisualizer(km)\n","\n","    visualizer.fit(X) # Fit the training data to the visualizer\n","    visualizer.poof() # Draw/show/poof the data"],"metadata":{"id":"mr9GRVh49D5C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From this chart we can say that optimal number of cluster is 5. Because the silhouette score is highest for the cluster 5."],"metadata":{"id":"7BAQYGEEC_wT"}},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7AN1z2sKpx6M"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Here we define the number of clusters basis on the Silhouette Cofficient"],"metadata":{"id":"xIY4lxxGpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Number of cluster is defined by Silhouette Coefficient.\n","\n","Silhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1.\n","\n","1: Means clusters are well apart from each other and clearly distinguished.\n","\n","0: Means clusters are indifferent, or we can say that the distance between clusters is not significant.\n","\n","-1: Means clusters are assigned in the wrong way.\n","\n","The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b). To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of."],"metadata":{"id":"Su6i804j9ODW"}},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"9PIHJqyupx6M"}},{"cell_type":"code","source":["# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# We decided the number of clusters basis on the some rules and analysis of the graph"],"metadata":{"id":"eSVXuaSKpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["Here we got Silhouette Coefficient for optimal number of clusters. From this data we got optimal number of clusters is 5 because it has a higher Silhouette Coefficient"],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"markdown","source":["Here we cannot directly predict the number of cluster . After using this method we can get optimal number of clusters and we can implement it directly in data."],"metadata":{"id":"MzVzZC6opx6N"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["Silhouette score is the best evaluation metric for optimization the number of clusters.\n","\n","The optimal number of cluster gives us the lightness and transparency of the business.\n","\n","Through cluster we can find out which type of customers are in our data.\n","\n","This evaluation metric makes business decision easier. Getting the Silhouette score is very easy."],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["From the charts we can see that K-Mean Clustering model is best model for our data.\n","\n","Here we get optimal number of clusters is 6, but often the number of clusters is already determined within the business. If the number of clusters within a business is already determined, we can apply the algorithm well.\n","\n","Within the K-Mean Cluster graph we can see that the clusters are well divided.\n","\n","Through this cluster we can know what type of data is in which cluster.\n","\n","The goal of this problems may be to discover groups of similar examples within the data.\n","\n","The primary function of this algorithm is to perform segmentation, whether it is store, product, or customer. Customers and products can be clustered into hierarchical groups based on different attributes."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"code","source":["# Count Plot Visualization Code for number of movies and tv shows in each cluster\n","# Set labels\n","plt.figure(figsize=(12,6))\n","graph = sns.countplot(x='K_mean_cluster',data=data, hue='type', palette=['#564d4d', '#db0000'])\n","plt.title('Number of Movies and TV shows in each cluster - K-Means Clustering')\n","plt.xlabel('Kmeans Clusters')\n","\n","# Adding value count on the top of bar\n","for p in graph.patches:\n","   graph.annotate(format(p.get_height(), '.0f'), (p.get_x(), p.get_height()), xytext = (0,3), textcoords = 'offset points')"],"metadata":{"id":"XecZ0hOQ9m-e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's do Cluster Analysis....................\n","*  Clustering similar content by matching text-based features"],"metadata":{"id":"xzdEgSvF-I3F"}},{"cell_type":"code","source":["# WordCloud Plot Visualization Code for User Rating Review\n","# Define a Function for Clustering Similar Content by Matching Text-Based Features\n","def kmeans_worldcloud(cluster_num):\n","\n"," # Create a String to Store All The Words\n","  comment_words = ''\n","\n","  # Remove The Stopwords\n","  stopwords = set(STOPWORDS)\n","\n","  # Iterate Through The Column\n","  for val in data[data['K_mean_cluster']==cluster_num].tags.values:\n","\n","      # Typecaste Each Val to String\n","      val = str(val)\n","\n","      # Split The Value\n","      tokens = val.split()\n","\n","      # Converts Each Token into lowercase\n","      for i in range(len(tokens)):\n","          tokens[i] = tokens[i].lower()\n","\n","      comment_words += \" \".join(tokens)+\" \"\n","\n","  # Set Parameters\n","  wordcloud = WordCloud(width = 1000, height = 500,\n","                  background_color ='white',\n","                  stopwords = stopwords,\n","                  min_font_size = 10,\n","                  max_words = 1000,\n","                  colormap = 'gist_heat_r').generate(comment_words)\n","\n","  # Set Labels\n","  plt.figure(figsize = (6,6), facecolor = None)\n","  plt.title(f'Most Important Words In Cluster {cluster_num}', fontsize = 15, pad=20)\n","  plt.imshow(wordcloud)\n","  plt.axis(\"off\")\n","  plt.tight_layout(pad = 0)\n","\n","  # Display Chart\n","  plt.show()"],"metadata":{"id":"h-w15Pi8-Oem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WordCloud for cluster 0\n","kmeans_worldcloud(0)"],"metadata":{"id":"-QVJ6TH0-Rpu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 0: united states, documentaries, international movies etc."],"metadata":{"id":"oYNL7NYL-U4y"}},{"cell_type":"code","source":["# WordCloud for cluster 1\n","kmeans_worldcloud(1)"],"metadata":{"id":"gPNEa7zB-W-6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 1: action adventure, united states, international movies etc."],"metadata":{"id":"_O8Ckb2t-eFe"}},{"cell_type":"code","source":["# WordCloud for cluster 2\n","kmeans_worldcloud(2)"],"metadata":{"id":"RFsOK0FS-ZQZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 2: united states, standup comedy, comedian, tv etc."],"metadata":{"id":"LabJwbQ1-hGr"}},{"cell_type":"code","source":["# WordCloud for cluster 3\n","kmeans_worldcloud(3)"],"metadata":{"id":"8jdBA0FD-jX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 3: international tv, tv show, drama, crime tv etc."],"metadata":{"id":"ukXJpDr9-p_G"}},{"cell_type":"code","source":["# WordCloud for cluster 4\n","kmeans_worldcloud(4)"],"metadata":{"id":"6_pFCb2O-0gP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 4: international movies, drama, comedies, romantic etc."],"metadata":{"id":"5n1M_JbZ_JCL"}},{"cell_type":"code","source":["# WordCloud for cluster 5\n","kmeans_worldcloud(5)"],"metadata":{"id":"6i7F1xX__Mix"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 5: united states, children, family, dramas, independent movies etc."],"metadata":{"id":"x8_KAcBD_Ph5"}},{"cell_type":"markdown","source":["4.  Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"e17oqdAi_WZQ"}},{"cell_type":"code","source":["# Here i have use topic modeling instead of feature importance and model explainability.\n","# Model explainability does majory help in classification problem but here is the project of unsupervised ML.\n","# In topic modeling, we can get topic wise feature importance."],"metadata":{"id":"dp72fMLf_eYt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here i have use topic modeling. Assume that the clusters are topics. Here for topic modeling i use CountVectorizer process for Vectorization of data and i use Latent Dirichlet Allocation for building a topic."],"metadata":{"id":"OijZWN0d_imc"}},{"cell_type":"code","source":["# Use count vectorization process for our data\n","# Create a count vectorizer object\n","count_vectorizer = CountVectorizer()\n","\n","# Fit the count vectorizer using the text data\n","document_term_matrix=count_vectorizer.fit_transform(data['tags'])"],"metadata":{"id":"2wn9fEPh_lsX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LDA model\n","\n","from sklearn.decomposition import LatentDirichletAllocation\n","lda = LatentDirichletAllocation(n_components=6)\n","lda.fit_transform(document_term_matrix)"],"metadata":{"id":"RidDLqmD_n1t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Most Important Features for Each Topic\n","vocab = count_vectorizer.get_feature_names_out()\n","\n","for i, comp in enumerate(lda.components_):\n","    vocab_comp = zip(vocab, comp)\n","    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:5]\n","    print(\"Topic \"+str(i)+\": \")\n","    for t in sorted_words:\n","        print(t[0],end=\" \")\n","    print(\"\\n\")"],"metadata":{"id":"jZTSWLB0_sM4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate WordCloud Images for Given Topics\n","# Define a Function for Visualize Most Important Features for Each Topic\n","def draw_word_cloud(topic_num):\n","\n","  # Create a String to Store All The Words\n","  imp_words_topic=\"\"\n","\n","  # Set Parameters\n","  comp=lda.components_[topic_num]\n","  vocab_comp = zip(vocab, comp)\n","  sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:25]\n","  for word in sorted_words:\n","    imp_words_topic=imp_words_topic+\" \"+word[0]\n","\n","  # Set Parameters\n","  wordcloud = WordCloud(width = 1000, height = 500,\n","                  background_color ='white',\n","                  stopwords = stopwords,\n","                  min_font_size = 10,\n","                  max_words = 1000,\n","                  colormap = 'gist_heat_r').generate(imp_words_topic)\n","\n","  # Set Labels\n","  plt.figure(figsize = (6,6), facecolor = None)\n","  plt.title(f'Most Important Features in Topic {topic_num}', fontsize = 15, pad=20)\n","  plt.imshow(wordcloud)\n","  plt.axis(\"off\")\n","  plt.tight_layout(pad = 0)\n","\n","  #Display Chart\n","  plt.show()"],"metadata":{"id":"MfxIbcWv_xSi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WordCloud for Topic 0\n","draw_word_cloud(0)"],"metadata":{"id":"gohEkgjl_0sI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WordCloud for Topic 1\n","draw_word_cloud(1)"],"metadata":{"id":"Bq1_mq-O_5VL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WordCloud for Topic 2\n","draw_word_cloud(2)"],"metadata":{"id":"AgnPt6wv_7Lo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WordCloud for Topic 3\n","draw_word_cloud(3)"],"metadata":{"id":"Q04Bi8H4_9ZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WordCloud for Topic 4\n","draw_word_cloud(4)"],"metadata":{"id":"Yj15UtvJ__n0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WordCloud for Topic 5\n","draw_word_cloud(5)"],"metadata":{"id":"tFUqsWYRABog"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here from those wordcloud plots we can know which word is important for which cluster. We can also decide the name of Topics (Clusters) from this plots."],"metadata":{"id":"alZWxMNyAGGe"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["# Save the File\n","# Save the File\n","import pickle\n","\n","# Serialize process (wb=write byte)\n","# Save the best model (KMeans Clustering)\n","pickle.dump(kmean,open('kmeans_model.pkl','wb'))"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data.\n","# Unserialize process (rb=read byte)\n","pickled_model= pickle.load(open('kmeans_model.pkl','rb'))\n","\n","# Predicting the unseen data\n","pickled_model.predict(X)"],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_kmean"],"metadata":{"id":"GgCFb3noHLGK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["1.In this project, we worked on a text clustering problem where in we had to classify/group the Netflix shows into certain clusters such that the shows within a cluster are similar to each other and the shows in different clusters are dissimilar to each other.\n","\n","2.The dataset contained about 7787 records, and 12 attributes. We began by dealing with the dataset's missing values and doing exploratory data analysis (EDA).\n","\n","3.It was found that Netflix hosts more movies than TV shows on its platform, and the total number of shows added on Netflix is growing exponentially. Also, majority of the shows were produced in the United States, and the majority of the shows on Netflix were created for adults and young adults age group.\n","\n","4.Once obtained the required insights from the EDA, we start with Pre-processing the text data by removing the punctuation, and, stop words. This filtered data is passed through TF - IDF Vectorizer since we are conducting a text-based clustering and the model needs the data to be vectorized in order to predict the desired results.\n","\n","5.It was decided to cluster the data based on the attributes: director, cast, country, genre, and description. The values in these attributes were tokenized, preprocessed, and then vectorized using TFIDF vectorizer.\n","\n","6.Through TFIDF Vectorization, we created a total of 20000 attributes. We used Principal Component Analysis (PCA) to handle the curse of dimensionality. 4000 components were able to capture more than 80% of variance, and hence, the number of components were restricted to 4000. We first built clusters using the k-means clustering algorithm, and the optimal number of clusters came out to be 6. This was obtained through the elbow method and Silhouette score analysis.\n","\n","7.Then clusters were built using the Agglomerative clustering algorithm, and the optimal number of clusters came out to be 12. This was obtained after visualizing the dendrogram.\n","\n","8.A content based recommender system was built using the similarity matrix obtained after using cosine similarity. This recommender system will make 10 recommendations to the user based on the type of show they watched."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}